{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face_Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMK5dUVWvtaA1fWtFGgXyN6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sherifmost/face-recognition/blob/main/Face_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlMQSGkWuRhC"
      },
      "source": [
        "# **About the project**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlWcBMiNuYn4"
      },
      "source": [
        "This is a project in the information systems and software course. Our objective is to perform face recognition on the ORL dataset and test its accuracy using PCA and LDA along with KNN classifiers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoWtEudgurgr"
      },
      "source": [
        "# **About the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii2iyB4Hutuu"
      },
      "source": [
        "We used the ORL data set for face recognition, which contains 40 different subjects with 10 images each. Each image is a 92x112 image in PGM (Portable Gray Map) format. Images are classified by being placed in different directories; where those in folder sx belong to subject number x(x between 1 and 40). An image for a certain subject is named Y.pmg where Y is the image number (between 1 and 10).\n",
        "Credits to: *AT&T Laboratories Cambridge* for providing the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HlnhRnDvc97"
      },
      "source": [
        "# **Needed library imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-5KLVtTvxz3"
      },
      "source": [
        "from google.colab import drive\n",
        "# used to manipulate the folders containing the images and read them out\n",
        "import os\n",
        "import matplotlib.image as mpimg \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import linalg as lg \n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5BOIf4Gvy1O"
      },
      "source": [
        "# **Labels and constants**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9mMSXtkv5Q8"
      },
      "source": [
        "# file paths\n",
        "path_data = '/content/drive/My Drive/Information systems/Assignment 1/Data set';\n",
        "\n",
        "# symbols\n",
        "delim = '/';\n",
        "\n",
        "# image dimensions\n",
        "image_len = 92;\n",
        "image_width = 112;\n",
        "\n",
        "# constant numbers\n",
        "training = 1;\n",
        "testing = -1;\n",
        "num_subjects = 40;"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcphDZSL6gOh"
      },
      "source": [
        "# **Helper functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR3Onf9Lx411"
      },
      "source": [
        "## Helper functions for manipuating data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK0_u8y757BS"
      },
      "source": [
        "# functions used as keys for sort function\n",
        "def numeric_key_folders(x):\n",
        "  return int(x[1:]);\n",
        "def numeric_key_images(x):\n",
        "  # we want to get the number till .pmg so remove last 4 characters from the string considered\n",
        "  return int(x[0:(len(x)-4)]);"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29nbI6gIx_JJ"
      },
      "source": [
        "## Helper functions for LDA dimensionality reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds4DF3hjps5r"
      },
      "source": [
        "# function calculates the class means (mean of each class) given an array containing the number of samples per class (assuming the data matrix is sorted accordingly)\n",
        "def get_class_means(D,num_samples):\n",
        "  means = [];\n",
        "  # keep the begining of the class from which the mean is calculated\n",
        "  class_begin = 0;\n",
        "  for curr_num in num_samples:\n",
        "    curr_num = int(curr_num);\n",
        "    means.append(np.mean(D[class_begin : class_begin + curr_num,:],axis = 0));\n",
        "    class_begin = class_begin + curr_num;\n",
        "  return np.array(means);\n",
        "# function that calculates the S_b matrix given the number of samples for each class, the class means and the overall mean\n",
        "def get_S_b(num_samples,class_means):\n",
        "  # calculating the overal sample mean\n",
        "  overal_mean = np.mean(class_means,axis = 0);\n",
        "  # S_b has dimensions same as B which are d x d (where d is number of dimensions which is same as shape of the image after flattening)\n",
        "  S_b = np.zeros(shape = (overal_mean.shape[0],overal_mean.shape[0]));\n",
        "  # looping to calculate S_b\n",
        "  for i in range(0,num_samples.shape[0]):\n",
        "    S_b = S_b + num_samples[i] * np.dot((class_means[i,:] - overal_mean).reshape(overal_mean.shape[0],1),(class_means[i,:] - overal_mean).reshape(1,overal_mean.shape[0]));\n",
        "  return S_b;\n",
        "# function that centers the data given the data matrix, the class means and number of samples per class\n",
        "def get_data_centered(D,num_samples,class_means):\n",
        "  data_centered = [];\n",
        "  class_begin = 0;\n",
        "  mean_location = 0; \n",
        "  for curr_num in num_samples:\n",
        "    curr_num = int(curr_num);\n",
        "    data_centered.append(D[class_begin : class_begin + curr_num,:] - class_means[mean_location,:]);\n",
        "    mean_location = mean_location + 1;\n",
        "    class_begin = class_begin + curr_num;\n",
        "  return np.array(data_centered).reshape(D.shape[0],D.shape[1]);\n",
        "# function that obtains S matrix given the centered data and the number of samples per class\n",
        "def get_S(D_centered,num_samples):\n",
        "  S = np.zeros(shape = (D_centered.shape[1],D_centered.shape[1]));\n",
        "  class_begin = 0;\n",
        "  for curr_num in num_samples:\n",
        "    curr_num = int(curr_num);\n",
        "    S = S + D_centered[class_begin : class_begin + curr_num].T @ D_centered[class_begin : class_begin + curr_num];\n",
        "    class_begin = class_begin + curr_num;\n",
        "  return S;\n",
        "# function given a matrix returns the n dominant eigen vectors\n",
        "def get_dom_eig_vec(mat,n):\n",
        "  # Getting eigen values and eigen vectors\n",
        "  # As we don't know whether mat is symmetric or not, eig is used.\n",
        "  eig_val,eig_vec = lg.eig(mat);\n",
        "  # As dominant eigen vectors should be taken according to the magnitude of eigen values \n",
        "  # (as negative values only indicate reverse of the vector direction), we should consider the\n",
        "  # absolute value of the eigen values.\n",
        "  eig_val = np.absolute(eig_val);\n",
        "  # using argsort to get dominant eigen vectors according to largest eigen values\n",
        "  sorted_indecies = eig_val.argsort()[::-1];\n",
        "  eig_vec = eig_vec[:,sorted_indecies];\n",
        "  # when checking the results, eigen vectors may include imaginary parts.\n",
        "  # we are only concerned with the real parts\n",
        "  eig_vec = np.real(eig_vec);\n",
        "  # getting first n dominant eigen vectors to be the proection matrix\n",
        "  P = eig_vec[:,:n];\n",
        "  return P;\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctPEnjwTv6Fh"
      },
      "source": [
        "# **Obtaining the data and cleaning it**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv7Uwx6IwUGo",
        "outputId": "86226c47-f7de-4bc5-c83f-5ba6b482ff66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# I uploaded the data to google drive as a zip file in order use it here\n",
        "# Mounting the drive\n",
        "drive.mount('/content/drive/')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heaEJiCUxdup"
      },
      "source": [
        "# unzipping the file, to be run only once\n",
        "!unzip '/content/drive/My Drive/Information systems/Assignment 1/orl_dataset.zip' -d '/content/drive/My Drive/Information systems/Assignment 1/Data set'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1av0tG2l4Bdh"
      },
      "source": [
        "## Reading the data to generate the data matrix and the label vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkkTLblD3_ot"
      },
      "source": [
        "def generate_data():\n",
        "  # first obtaining the directories and sorting them\n",
        "  subjects_dir = os.listdir(path_data);\n",
        "  # Note that I manually removed the readme file from my drive after unzipping the data\n",
        "  # sorting the directories to obtain the subjects' data sorted from 1 to 40\n",
        "  subjects_dir.sort(key = numeric_key_folders);\n",
        "  # converting the images to the flattened format and filling the D and Y matrices as required\n",
        "  D = [];\n",
        "  Y = [];\n",
        "  flatten_dim = image_len * image_width;\n",
        "  for current_dir in subjects_dir:\n",
        "    current_label = numeric_key_folders(current_dir);\n",
        "    subject_images = os.listdir(path_data + delim + current_dir);\n",
        "    # sorting the images to obtain the current subject's images sorted from 1 to 10\n",
        "    subject_images.sort(key = numeric_key_images);\n",
        "    for current_image in subject_images:\n",
        "      # image is reshaped to be flattened as a vector\n",
        "      D.append(mpimg.imread(path_data + delim + current_dir + delim + current_image).reshape(flatten_dim));\n",
        "      Y.append(current_label);\n",
        "  return np.array(D), np.array(Y);"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CRaR7r6Beij"
      },
      "source": [
        "## splitting the data and labels to training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om8tcs_3Bm_r"
      },
      "source": [
        "# this function splits the data according to specified values to take which for training and which for testing.\n",
        "# to get odd rows for training and even rows for testing, make train_each = test_each = 1 and start = testing (as matrices and vectors are 0 indexed).\n",
        "def split_data(D,Y,train_each = 1,test_each = 1,start = testing):\n",
        "  # flag checks whether data is in training or testing\n",
        "  destination = start;\n",
        "  # counter checks how many samples were taken\n",
        "  taken = 0;\n",
        "  D_train = [];\n",
        "  Y_train = [];\n",
        "  num_samples_train = np.array(np.zeros(num_subjects));\n",
        "  D_test = [];\n",
        "  Y_test = [];\n",
        "  num_samples_test = np.array(np.zeros(num_subjects));\n",
        "  for i in range(0, Y.shape[0]):\n",
        "    taken = taken + 1;\n",
        "    if destination == training:\n",
        "      D_train.append(D[i,:]);\n",
        "      Y_train.append(Y[i]);\n",
        "      num_samples_train[Y[i] - 1] = num_samples_train[Y[i] - 1] + 1; \n",
        "      if taken == train_each:\n",
        "        destination = testing;\n",
        "        taken = 0;\n",
        "    else:\n",
        "      D_test.append(D[i,:]);\n",
        "      Y_test.append(Y[i]);\n",
        "      num_samples_test[Y[i] - 1] = num_samples_test[Y[i] - 1] + 1; \n",
        "      if destination == testing:\n",
        "        destination = training;\n",
        "        taken = 0;\n",
        "  return np.array(D_train),np.array(Y_train),num_samples_train,np.array(D_test),np.array(Y_test),num_samples_test;    \n",
        "        \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N4ccEw8oZCD"
      },
      "source": [
        "# **Dimensionality reduction using LDA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYEmq3Jew34Q"
      },
      "source": [
        "def reduce_lda(D_train,D_test,num_samples_train):\n",
        "  # getting the class means for the training data\n",
        "  means_class_train = get_class_means(D_train,num_samples_train);\n",
        "  # getting S_b (which replaces the between-class scatter matrix B in case of multiclass LDA)\n",
        "  S_b_train = get_S_b(num_samples_train,means_class_train); \n",
        "  # getting S using centered data and number of samples\n",
        "  S_train = get_S(get_data_centered(D_train,n_train,means_class_train),n_train);\n",
        "  # getting S^-1 * S_b\n",
        "  # pinv is used to overcome numerical errors (by approximation)\n",
        "  # when trying this, I found that results are not correct (numbers don't make sense) unless we identify the matrix to be \n",
        "  # hermitian which corresponds to the S matrix being symmetric by nature.\n",
        "  mul_res_train = lg.pinv(S_train,hermitian = True) @ S_b_train;\n",
        "  # getting the dominant m - 1 eigen vectors where m is number of classes as the projection matrix\n",
        "  P = get_dom_eig_vec(mul_res_train,num_samples_train.shape[0] - 1);\n",
        "  # finally we just return the data in reduced form\n",
        "  return np.real(D_train @ P), np.real(D_test @ P);\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I2Fi4-31CXo"
      },
      "source": [
        "# **Classification using KNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lku37VpT1I7P"
      },
      "source": [
        "# given the train data, train labels, test data, test labels and number of neighbours \n",
        "# it returns the test accuracy using KNN with this number of neighbours.\n",
        "# for the tie breaking, we decided to keep the default strategy.\n",
        "# for weights we used distance as a parameter, so that the nearer the neighbour the more impact it has\n",
        "# on the classification.\n",
        "def classify_KNN(D_train,Y_train,D_test,Y_test,n_neigh = 1):\n",
        "  classifier = KNeighborsClassifier(n_neighbors = n_neigh, weights = 'distance');\n",
        "  classifier.fit(D_train,Y_train);\n",
        "  acc_test = classifier.score(D_test, Y_test);\n",
        "  return acc_test;"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4G0ZQd9n7N0"
      },
      "source": [
        "# **Scripts used to run the function and give the required outputs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC6pOiUPC5PE"
      },
      "source": [
        "# script to generate the data and split it\n",
        "D,Y = generate_data(); \n",
        "D_train,Y_train,n_train,D_test,Y_test,n_test = split_data(D,Y);"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vio3IJMcuZQ8"
      },
      "source": [
        "# script to perform the LDA reduction\n",
        "D_train_reduced,D_test_reduced = reduce_lda(D_train,D_test,n_train);"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRrBDtZH05En",
        "outputId": "72bd32f0-4199-41d8-af33-7440d7490d18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# script to get the KNN accuracy for LDA using first NN\n",
        "acc_test = classify_KNN(D_train_reduced,Y_train,D_test_reduced,Y_test);\n",
        "print('accuracy of LDA using first nearest neighbour:');\n",
        "print(acc_test);"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy of LDA using first nearest neighbour:\n",
            "0.96\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}